# AI Governance Tool - All Descriptions

This document contains all the descriptive text from the AI Governance Strategy Builder tool, organized by category for easy manual editing.

The Race Towards AGI

Great powers and AI labs have entered a race to develop AGI as quickly as possible. As the hero of this story, your role is to allocate global resources to ensure that humanity survives the dangers of misalignment, misuse or gradual disempowerment that AI scholars have warned about.

You will pick what global posture to take, which institutions to build, and which mechanisms we need to implement this.

First, which world are we in?

## Difficulty Levels

### Optimist Mode (95°)
We live in the optimistic world where AI is aligned-by-default. With proper coordination and reasonable precautions, we can navigate this transition successfully. Multiple approaches can work, and humanity has broad margin for error.

### Moderate Mode (70°)
We're in the world where things could easily go wrong. Making AI go well requires serious effort and coordination, but multiple strategies could succeed if executed well. 

### Pessimist Mode (35°)
We live in the precarious world where AI development is fraught with dangers. Only careful, well-executed strategies have a chance of success. Most approaches will fail without exceptional coordination and foresight.

### Yudkowsky Mode (5°)
We live in the Yudkowskyian world where AI alignment is extraordinarily difficult, and doom is the default scenario. Only a narrow path can lead us to existential safety, and humanity must thread the needle perfectly.

## Resource Levels

Mobilizing Global Resources

The stakes couldn't be higher. World leaders have acknowledged the existential importance of getting AI governance right, but haven't yet committed the necessary resources.

The level of commitment varies dramatically based on public awareness, political will, and competing priorities.

**How much support have you secured for this critical mission?**

### High Commitment Scenario
**$350B Budget • 40 Political Capital** Major powers have recognized AI as the defining challenge of our time. Unprecedented international cooperation and public support have enabled resource investment comparable to wartime mobilization. 

### Moderate Commitment Scenario
**$220B Budget • 24 Political Capital.** Some countries are taking AI governance seriously, but competing priorities and disagreements limit the resources that can be dedicated to making AI go well. 

### Low Commitment Scenario
**$140B Budget • 16 Political Capital.** Most governments are distracted by petty feuds and the latest cause du jour. AI safety is viewed as a luxury or future problem. You must work within severe constraints and limited international cooperation.

## Strategic Postures

Choose Your Strategic Approach

As the architect of global AI governance strategy, you must first choose the fundamental approach that will guide all other decisions.

Select the strategic posture that best represents your vision for managing AI risks:

### Laissez-faire
A "permissionless innovation" approach that lets AI models proliferate without global coordination. This strategy relies on market forces and responsible corporate behaviour to naturally regulate development, similar to early internet policy in the 1990s. 

### AI clubs / blocs
Developing networks between aligned countries for AI coordination. This approach creates powerful coalitions that can share safety research and establish common standards. 

### Open Global Investment (OGI)
Use market-based mechanisms to concentrate investment in certain actors, creating massive incentive structures to channel the world's best talent toward safe AI development. This approach, proposed by Nick Bostrom, aims to eliminate dangerous race dynamics by coordinating global funding.

### Mutually Assured Destruction/AI Malfunction (MAD/MAIM)
This approach focuses on working towards a deterrence-based equilibrium similar to Cold War nuclear deterrence. Nations develop verification systems and maintain credible AI-based deterrence, so that, if major power attempt a "first strike" using AI, the other party/parties will be able to cause their AI systems to malfunction. 

### Global Moratorium
A coordinated halt to AGI development worldwide, similar to nuclear test ban treaties or biological research moratoria. This aims to provide breathing room for researchers to solve key alignment problems before (possibly) restarting AI research under strict safety protocols. 

### Cooperative development
International laws ensure defensive AI technologies are aligned with humans are developed before offensive technologies. This approach, supported by frameworks like Anthropic's safety commitments, unites humanity in building safe AI through unprecedented collaboration.

### d/Acc (defensive accelerationism)
This approach focuses on 4 D's - defensive, decentralized, differential development. Essentially, developing safety technologies more quickly than dangerous technologies. Supporting this strategy focuses on empowering a wide range of actors to develop effective safety and AI control technologies.

### Non-proliferation
Stop proliferation of AI capabilities and prevent diffusion of dangerous models or capabilities, similar to the Nuclear Non-Proliferation Treaty. This approach uses export controls and licensing to limit the number of actors who could pose existential risks, making governance much more manageable. 

### Strategic advantage
Make sure one responsible actor or nation state "wins" by developing advanced AI systems faster than geopolitical rivals. This approach is intended to ensure that safety-conscious developers reach AGI before less careful competitors, preventing a dangerous multipolar scenario. The leading actor could then implement global safety standards from a position of strength.

## Institutional Architectures

### Self-governance
Firms introduce voluntary standards for AGI governance. This approach relies on industry initiative like the Frontier Safety Frameworks from the 2024 Seoul Summit. 

### Benefits & access distribution
Institutions for distributing the benefits of AI or access to transformative AI systems to all parties, such as through windfall clauses proposed by Nick Bostrom. 

### Corporate governance bodies
Board-level structures for AI risk, either enforced or encouraged by regulators. Similar to Sarbanes-Oxley compliance boards or ESG mandates, these would ensure corporate oversight of AI development risks. 

### International AI Safety Agency
A regulatory global agency with international buy-in, similar to the IAEA for nuclear materials. This would provide centralized enforcement of global AI safety standards with real regulatory teeth.

### Scientific consensus (IPCC-for-AI)
An international scientific consensus body like the Intergovernmental Panel on Climate Change, designed to provide authoritative scientific assessments of AI risks and capabilities. Proposals include Carnegie's GAIO (Global AI Observatory).

### Political forum (UNFCCC-style)
Ongoing norm-setting and review cycles similar to the UN Framework Convention on Climate Change, providing a platform for continuous international negotiation on AI governance. China has recently floated proposals for a "UNFCAI" at the UN. 

### Emergency response hub
Rapid, cross-jurisdiction incident response capabilities, analogous to the IAEA Incident Centre or WHO pandemic alert systems. This would enable coordinated response to AI safety incidents across national boundaries. 

### Independent national regulator
Establish independent regulators to regulate, monitor and audit AI companies and models, similar to existing regulatory bodies in telecommunications and the energy sector. 

### Coordination of policy & regulation
Create a coordination layer for policy and regulation, starting with information-sharing between national AI safety institutes, similar to the Financial Stability Board for banking. Currently ongoing through the G7 Hiroshima Process, INAISE network, and Global AI Governance Initiative. 

### Domestic AI regulators (existing)
Support national-level agencies using existing regulatory structures, such as the EU AI Act implementation through national agencies, China's CAC interim measures, or the UK's multi-regulator taskforce. 

### International Joint Research (CERN for AI)
Joint, multipolar development organization for AGI research, similar to CERN for particle physics or ITER for nuclear fusion. This would enable shared international research while managing risks through collective oversight. 

### Embedding in existing institutions
Using existing institutions for AI governance by adding "bolt-on" provisions to organizations like the WTO, ISO/IEC standards bodies, or UN agencies. This leverages established diplomatic channels and expertise. It's already happening through ISO/IEC AI committees and G7 processes.

## Regulatory/Legal Mechanisms

Establish Legal & Regulatory Framework

Your institutions need teeth. Legal mechanisms, compliance frameworks, and regulatory tools are needed to enforce your governance vision.

These are the rules of the game that will shape how AI systems are developed, deployed, and monitored. 

Select the regulatory and legal mechanisms you want to implement:

### Auditor certification regimes
Certification schemes to train and verify auditing bodies, similar to financial audit firms (Big Four) or ISO 27001 auditors. The EU AI Act sketches auditor roles, and industry coalitions like the Cloud Security Alliance are developing frameworks. 

### Liability mechanisms
Civil or criminal responsibility for certain AI-related harms, building on existing product liability law like the EU Product Liability Directive. This creates legal accountability for AI companies when their systems cause damage. 

### Whistleblower protections
Instituting protections at a domestic level for whistleblowers at AI labs or other AI infrastructure organizations, similar to US/UK whistleblower laws (Sarbanes-Oxley, SEC). The "Right To Warn" initiative and EU AI Act mention such protections.

### Market-shaping mechanisms
Use mechanisms like prizes and advance market commitments to incentivize broad dissemination of safety features, similar to AMCs for vaccines or clean-tech subsidies. This approach leverages market incentives to promote safety research and deployment. 

### Frontier Safety Frameworks
Frontier labs pledge voluntarily to abide by certain safety principles, similar to Responsible Care chemical industry pledges. These voluntary commitments were made at 2023-24 AI Summits (UK, Seoul) by leading AI companies.

### Pre-deployment evaluation
Safety tests, evaluations, and red-teaming before AI system deployment, similar to FDA drug trials or product safety testing. Referenced in the EU AI Act, US voluntary commitments, and Anthropic's RSPs. 

### Mandatory transparency reports
Labs are obliged to submit regular reports to a national or international body, similar to environmental reporting mandates. The EU AI Act requires transparency filings, and various US proposals exist.

### Sector-specific prohibitions
Limiting use of AI in specific high-risk sectors like biotech, military applications, or autonomous weapons, similar to bioweapons bans. The EU AI Act includes high-risk domain prohibitions, and NGO campaigns like "Stop Killer Robots" advocate for such restrictions. 

### Incident reporting registry
Shared error disclosure system similar to aviation safety reporting systems, cybersecurity breach notifications, or IAEA incident reporting. The OECD and EU are working on harmonized AI incident reporting frameworks. 

### Model registry
Registering AI models and their use-cases, similar to chemical inventories or aircraft registries. EU AI Act Article 49 establishes this framework, with additional proposals from governance researchers. 

### Standard setting
Voluntary norms to encourage industry standards through bodies like ISO, IEC, or W3C. Currently happening through ISO/IEC AI committees, G7 Hiroshima guidelines, and the ISO 42001 standard. 

### Staged capability thresholds
Mechanisms to agree to pause development or tighten rules at specific capability milestones, similar to arms-control treaties tied to missile counts or Basel banking accords. There's increasing traction for compute thresholds in policy discussions and the EU AI Act debate. 

### Licensing
Approvals for AI models based on certain requirements, such as compute thresholds or capability tests, similar to aviation licenses or nuclear plant licensing. The EU AI Act includes prototypes of this approach, and China's CAC has a licensing regime. 

## Technical & Infrastructural Controls

### Energy/Power-use monitoring
Using monitoring of utilities to observe high power usage associated with GPU clusters, similar to industrial power audits or cryptocurrency mining crackdowns. Proposed in "Compute at Scale" research, with some Chinese provinces already monitoring power anomalies.

### Kill-switch protocols
Technical mechanisms to enable emergency suspension of training runs or inference processes, similar to nuclear "permissive action links" or emergency stop systems in aviation. Some mention exists in EU AI Act and safety pledges, but mostly conceptual with no technical standards. 

### Export controls
Limiting the export of advanced chips, compute resources, or specialized talent, similar to nuclear dual-use export controls or semiconductor sanctions (COCOM, Wassenaar). Currently active through US/EU/Japan controls on advanced GPUs and China imposing talent outflow restrictions. 

### Hardware-based verification
On-chip mechanisms to prevent misuse or enforce limitations, similar to Trusted Platform Modules (TPMs), Digital Rights Management (DRM), or IMEI device tracking. Early discussion exists in RAND/GovAI papers, with chipmakers experimenting with secure enclaves. 

### Cloud-based enforcement
Using cloud-based mechanisms to control access to certain hardware or processes (like model training or inference), similar to financial KYC requirements enforced through banks or App Store gatekeeping. Growing momentum exists through US/UK policies and GovAI "Governing through the Cloud" research, with pilots for compute usage logging. 

### Technical compute caps
Using physical limits on chips or architectures to prevent models above a certain size, similar to OPEC production quotas, emissions caps, or bandwidth throttling. Proposed by MIRI and others, with compute thresholds discussed in AI Act drafts and policy papers. 

### Software-based verification
Monitors training processes or inference through cryptography, proof-of-learning systems, or blockchain attestations, similar to software license verification, reproducible clinical trials, or financial transaction audit trails. 

---

*This document was generated from the AI Governance Strategy Builder tool for manual editing purposes.*